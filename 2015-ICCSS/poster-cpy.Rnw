%\documentclass[portrait,a0,posterdraft]{a0poster}
\documentclass[portrait,a0,final]{a0poster}
% For documentation, see
% ftp://ftp.funet.fi/pub/TeX/CTAN/macros/latex/contrib/a0poster/a0_eng.pdf

\usepackage{epsf,pstricks}
\usepackage[utf8]{inputenc}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
%\usepackage{hyperref}
\usepackage{geometry}
\geometry{verbose,tmargin=1.0cm,bmargin=1.5cm,lmargin=1.5cm,rmargin=1.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=ning,pdfborder={0 0 1},backref=ning,colorlinks=ning]
 {hyperref}
\hypersetup{pdfstartview={XYZ null null 1}}
\usepackage{authblk}
\usepackage{nopageno}
\usepackage{mathtools}
\usepackage{color}
\usepackage[rightcaption]{sidecap}
\usepackage{graphicx}

% Fonts
%\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\familydefault}{\rmdefault}
%\usepackage{times}

% For tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,snakes}
\usepackage{amsmath,amssymb}
\usetikzlibrary{positioning}

\title{\Huge Penalized regression inference regarding variable selection in high dimensions: presentation of selected methods implemented in R}
\date{}
\author{{Marta Karas}\\ \Large Contact: \textcolor{blue}{http://statsox.github.io}}

\begin{document}
% \SweaveOpts{concordance=TRUE}
\pagestyle{empty}
\maketitle
\Large


% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20, text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{greenbox} = [rectangle, draw=blue, fill=green!20, text width=5em, text centered, rounded corners, inner sep=10pt, inner ysep=12pt, very thick]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse, node distance=4cm, minimum height=2em, text width=5em]
\tikzstyle{mybox} = [draw=blue, fill=green!20, very thick, rectangle, rounded corners, inner sep=10pt, inner ysep=20pt]
\tikzstyle{myboxwhite} = [rectangle, rounded corners, inner sep=10pt, inner ysep=0pt]
\tikzstyle{myboxblue} = [rectangle, fill=blue!10, rounded corners, inner sep=10pt, inner ysep=20pt]
\tikzstyle{myboxviolet} = [rectangle, fill=violet!10, rounded corners, inner sep=10pt, inner ysep=20pt]
\tikzstyle{myboxyellow} = [rectangle, fill=yellow!10, rounded corners, inner sep=10pt, inner ysep=20pt]
\tikzstyle{fancytitle} = [fill=white, text=black, ellipse, draw=blue]


\vspace{0cm}

\begin{tikzpicture}[]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\node [myboxwhite] (intro){
    \begin{minipage}{0.45\textwidth}

{\bf Association between an outcome variable and predictors.} To assess the association between an outcome $y \in \mathbb{R}^n$ and a set of predictors $x_j \in \mathbb{R}^n$, $j = 1,...,p$, one might consider the model:
$$\mathbf{y} =  \mathbf{X} \beta + \epsilon,$$ 
where $\mathbf{X} = [\mathbf{x}_1, . . . , \mathbf{x}_p] \in \mathbb{R}^{n \times p}$, $\beta \in \mathbb{R}^p$ is vector of coefficients, and $\epsilon \in \mathbb{R}^n$ is a vector of errors with mean zero and constant variance. If  the number of variables $p$ is much smaller than $n$, we could perform a formal statistical test for whether an element of $\beta$ is zero using classical methods, such as likelihood ratio or Wald test. However, \textcolor{red}{in the high-dimensional setting, when the number of variables $p$ is large, these tests have low power, or are undefined}.

{\bf Penalized regression techniques.} In the case where $p$ is large, penalized regression techniques such as Ridge and Lasso can be employed to obtain $\beta$ estimates:
$$\widehat{\beta }_{\lambda} = \underset{b \in \mathbb{R}^p}{arg \; min}\left  \{ \frac{1}{2n} ||\mathbf{y - Xb}||^2_2 +\lambda J(b)\right  \},$$
where $J(b) = \frac{1}{2}||b||_2$ for Ridge and $J(b) = ||b||_1$ for Lasso.
However, \textcolor{red}{Lasso and Ridge yield biased estimators of  $\beta$, thus these procedures do not provide $p$-values or confidence intervals}.

    \end{minipage}
};



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\node [myboxblue, below of=intro, node distance=37cm, yshift=0cm, xshift=0cm] (methods){%
    \begin{minipage}{0.45\textwidth}

{\bf Methods. Penalized regression inference.} Here, we present examples of usage of a few selected methods available in \texttt{R}:

\begin{itemize}

\item \texttt{lassoscore \{lassoscore\}}: \textbf{Score test based on penalized regression}. Performs penalized regression of an outcome on all but a single feature, and test for correlation of the residuals with the held-out feature; applied on each feature in turn. 

\item \texttt{hdi \{hdi\}}: \textbf{Multi sample-splitting}. Splits the sample into two equal halves, $I_1$ and $I_2$. First half $I_1$ is used for variable selection (with the use of Lasso) and the second half $I_2$, with the reduced set of selected variables (from $I_1$), is used for "classical" statistical inference in terms of $p$-values. Repeats the splitting procedure $B$ times and aggregates obtained p-values. 

\item \texttt{grace.test \{Grace\}}: \textbf{Grace test}. Proposes how to overcome that Ridge is a biased estimator of $\beta$ and its estimation bias is negligible only if the Ridge tuning parameter $\lambda$ is close to zero. To construct a test statistic for the null hypothesis $H_0: \; \beta^*_j = 0$ for some $j \in \{1, ..., p\}$, it adjusts for the potential estimation bias by using a stochastic bound derived from an initial estimator. Since with this adjustment the tuning parameter $\lambda$ needs not be very small, coefficient estimation and corresponding $p$-values for penalized regression might be obtained.  

\end{itemize}

\end{minipage}

};


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\node [myboxviolet, below of=methods, node distance=32cm, yshift=0cm, xshift=0cm] (methods2){%
    \begin{minipage}{0.45\textwidth}

{\bf Methods. Assessing the inference results.} In regression settings, False
Discovery Proportion (FDP) is often used to describe the proportion of false "discoveries" (whose coefficients in the true {\it full model} are zero). However, in settings with the presence of correlated predictors, more than one variable is likely to be capturing the same underlying signal. \textcolor{red}{Then, "classical" FDP suffers from unintuitive and potentially undesirable behavior.}

\begin{itemize}

\item Here, we use \textbf{False Variable Proportion (FVP)} measure ([x]), which considers a variable to be an interesting selection if it captures signal that has not been explained by any other variable in the selected model. Mathematically, for a selected variables set $A \subseteq  \{1, . . . , p\}$, we project the mean $\mathbf{X}\beta$ from the {\it full model} onto subset of predictors $\mathbf{X}_A$ to obtain a projected mean $\mathbf{X}\beta^{(A)}$. We define a selected variable to be a false selection if it has a zero coefficient in this projected mean vector: 
$$FVP =|  \{ j \in A: \beta_j^{(A)} = 0 \} |/|A|.$$

\end{itemize}

\end{minipage}

};



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\node [myboxwhite, right of=intro, node distance=40cm] (fvillustration){%

  \begin{minipage}{0.48\textwidth}
  
\includegraphics[width=1.0\textwidth]{images/FVR-ill.png}
\small
Figure 1. \textbf{False Variable Proportion (FVP)} illustration. Variables are denoted as correct selections if they are capturing unique signal among the selected variables. Thus $B_2$ is correctly selected in the first set. However, $B_2$ is considered a false selection in the second set because it adds no information beyond $B_1$. Figure \& caption source: X. 

\end{minipage}
};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\node [myboxviolet, below of=fvillustration, node distance=9cm, yshift=0cm, xshift=0cm] (fvnote){%
    \begin{minipage}{0.48\textwidth}
    
{\it False Variable Proportion application.} In non-orthogonal settings, one is not likely to obtain exact zeros in a projected mean $\mathbf{X}\beta^{(A)}$ vector and heuristics is used to define threshold below which a variable is considered to be a false selection (here: $0.1$).  

\end{minipage}

};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\node [myboxyellow, below of=fvnote, node distance=11cm] (usageexample){%
  \begin{minipage}{0.48\textwidth}

\textbf{\texttt{R} usage example.} Assume we are given data matrix $X_{100 \times 300} \sim N(0, \Sigma )$, true signal $\beta$ and observed response variable $Y \sim N(X \beta, 1^2)$.

<<eval=TRUE, echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, include = FALSE>>=
rm(list=ls())
library(knitr)
library(MASS)
library(Matrix)
library(lassoscore)
library(hdi)
library(Grace)
library(glmnet)
library(ggplot2)
source("https://raw.githubusercontent.com/statsox/Penalized-Regression-Inference/master/R/vizumat.R")
source("https://raw.githubusercontent.com/statsox/Penalized-Regression-Inference/master/R/utils.R")
source("https://raw.githubusercontent.com/statsox/Penalized-Regression-Inference/master/R/false_variable_fw.R")

knitr::opts_chunk$set(echo = F, eval = T, message = F, warning = F, cache = T, fig = TRUE, global.par = TRUE)


FVP.proj.mean <- function(beta.true, beta.sel.idx, Sigma, error.sd = 1){
  p <- length(beta.true)
  A.plus.idx <- c(beta.sel.idx, p+1)
  
  Sigma.aug.11 <- Sigma
  Sigma.aug.12 <- Sigma %*% beta.true
  Sigma.aug.21 <- t(beta.true) %*% Sigma
  Sigma.aug.22 <- t(beta.true) %*% Sigma %*% beta.true + error.sd^2
  
  Sigma.aug.1. <- cbind(Sigma.aug.11, Sigma.aug.12)
  Sigma.aug.2. <- cbind(Sigma.aug.21, Sigma.aug.22)
  Sigma.aug <- rbind(Sigma.aug.1., Sigma.aug.2.)
  
  Sigma.aug.A.plus <- Sigma.aug[A.plus.idx, A.plus.idx]
  Sigma.aug.A.plus.inv <- solve(Sigma.aug.A.plus)
  Sigma.aug.A.plus.inv.Y <- Sigma.aug.A.plus.inv[(length(A.plus.idx)),]
  return(round(Sigma.aug.A.plus.inv.Y, 4))
}
@


<<eval=TRUE, echo = FALSE, cache = FALSE, fig=TRUE, fig.height=4, fig.width=5, fig.show='hold'>>=
base_size.gg <- 10

# Parameters 
p.tmp <- 200
n.tmp <- 100
sgnf.lvl <- 0.1
beta.sgnf.n <- 10

# Simulate data
set.seed(1)

Sigma <- toeplitz.mat(p.tmp, 0.1) 
X <- scale(mvrnorm(n.tmp, mu = rep(0, p.tmp), Sigma = Sigma))
beta <- rep(0, p.tmp)
beta[sample(1:p.tmp, beta.sgnf.n, replace = FALSE)] <- 1
X.beta <- X %*% beta
error.sd <- 1 
Y <- scale(rnorm(n.tmp, mean = X.beta, sd = error.sd))

# Sigma plot
vizu.mat(toeplitz.mat(p.tmp, 0.005) , "Sigma variance-covariance matrix", base_size = base_size.gg, geom_tile.colour = "white")

# True beta plot
plot.df <- data.frame(x = 1:p.tmp, y = beta)
plt <- 
  ggplot(plot.df, aes(x=x, y=y)) + 
  geom_line() + 
  labs(title = "true beta signal", x = "", y = "") + 
  theme_bw(base_size = base_size.gg, base_family = "Helvetica") 
plot(plt)

# Y ~ X %*% beta plot 
# vizu.mat(, "X data mat.", base_size = base_size.gg, geom_tile.colour = "white")
plot.df <- data.frame(x = X.beta, y = Y)
plt <- 
  ggplot(plot.df, aes(x=x, y=y)) + 
  geom_point() + 
  labs(title = "Y ~ X beta", x = "", y = "") + 
  theme_bw(base_size = base_size.gg, base_family = "Helvetica") 
plot(plt)
@


\end{minipage}
};


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\node [myboxwhite, below of=usageexample, node distance=11.5cm, yshift = -19cm] (usageexample2){%
  \begin{minipage}{0.48\textwidth}
  
\small

\textbf{\texttt{R}: Projected mean \& FVP.} Values "close" to 0 indicate false discovery. 

<<eval=TRUE, echo = TRUE, cache = TRUE, fig=TRUE, fig.height=4, fig.width=5, fig.show='hold'>>=
source("https://raw.githubusercontent.com/statsox/Penalized-Regression-Inference/master/R/false_variable_fw.R")

# True beta indices 
(beta.sel.idx <- which(beta != 0))
round(FVP.proj.mean(beta.true = beta, beta.sel.idx = beta.sel.idx, Sigma = Sigma), 2)

# Make change in selected beta indices vector we check for "false discovery" 
beta.sel.idx <- c(c(7), beta.sel.idx[-1])
round(FVP.proj.mean(beta.true = beta, beta.sel.idx = beta.sel.idx, Sigma = Sigma), 2)
@


\texttt{R}: $p$-values \& FVP: \textbf{Score test based on penalized regression.}

<<eval=TRUE, echo = TRUE, cache = TRUE, fig=TRUE, fig.height=4, fig.width=5, fig.show='hold'>>=
cv.res <-  cv.glmnet(X, Y) # Run cv.glmnet to choose *exemplary* lambda for which we compute lassoscore
res.lassoscore <- lassoscore(Y, X, lambda =  cv.res$lambda.1se) 
beta.selected.idx <- which(res.lassoscore$p.model < 0.1) # subset of p.values < 0.1
@

<<eval=TRUE, echo = FALSE, cache = FALSE, fig=TRUE, fig.height=2, fig.width=16, fig.show='hold'>>=
plot.discoveries(beta, beta.selected.idx)
@


\texttt{R}: $p$-values \& FVP: \textbf{Multi sample-splitting} 

<<eval=TRUE, echo = TRUE, cache = TRUE, fig=TRUE, fig.height=4, fig.width=5, fig.show='hold'>>=
res.hdi.multi <- hdi(X, Y, method = "multi.split", B = 50, model.selector = lasso.cv,
                     args.model.selector = list(nfolds = 10))
beta.selected.idx <- which(res.hdi.multi$pval.corr < 0.1)
@

<<eval=TRUE, echo = FALSE, cache = FALSE, fig=TRUE, fig.height=2, fig.width=16, fig.show='hold'>>=
plot.discoveries(beta, beta.selected.idx)
@


\texttt{R}: $p$-values \& FVP: \textbf{Grace test} 

<<eval=TRUE, echo = TRUE, cache = TRUE, fig=TRUE, fig.height=4, fig.width=5, fig.show='hold'>>=
lambda.2.seq <- exp(seq(-6, 10, length.out = 100)) 
res.grace <- grace.test(Y, X, L = matrix(0, p.tmp, p.tmp), lambda.L = 0, lambda.2 = lambda.2.seq)
beta.selected.idx <- which(res.grace$pvalue < 0.1)
@

<<eval=TRUE, echo = FALSE, cache = FALSE, fig=TRUE, fig.height=2, fig.width=16, fig.show='hold'>>=
plot.discoveries(beta, beta.selected.idx)
@

\end{minipage}
};





\end{tikzpicture}

\end{document}